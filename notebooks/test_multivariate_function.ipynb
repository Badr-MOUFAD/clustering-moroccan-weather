{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# set root directory\r\n",
    "import os\r\n",
    "\r\n",
    "os.chdir('../')\r\n",
    "os.getcwd()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\HP\\\\Desktop\\\\clustering-moroccan-weather-data'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# imports\r\n",
    "import numpy as np\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# load data\r\n",
    "data_beni_mellal = pd.read_csv(\"processed_data/processed_beni_mellal.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# # split function\r\n",
    "# # split according to weather variables\r\n",
    "# def isolate_data_col(data, col_name):\r\n",
    "\r\n",
    "#   frame = {}\r\n",
    "\r\n",
    "#   for year in data[\"crop_year\"].value_counts().sort_index().index:\r\n",
    "#     # select crop year\r\n",
    "#     query = data[\"crop_year\"] == year\r\n",
    "#     frame[year] = data[query][col_name].values\r\n",
    "\r\n",
    "\r\n",
    "#   # build data and then transpose\r\n",
    "#   data_cluster = pd.DataFrame(frame)\r\n",
    "#   data_cluster = data_cluster.T\r\n",
    "\r\n",
    "#   return data_cluster"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# # main function\r\n",
    "# def multivariate_clustering(data, nb_clusters=3, metric=\"euclidean\", linkage_method=\"ward\"):\r\n",
    "#     # prepare data: isolate data of each weather variable\r\n",
    "#     selected_cols = [\"cumulative_GDD\", \"cumulative_PRECTOT\", \"cumulative_RH2M\", \"cumulative_WS2M\"]\r\n",
    "#     dict_data = {}\r\n",
    "\r\n",
    "#     for col in selected_cols:\r\n",
    "#         dict_data[col] = isolate_data_col(data, col_name=col)\r\n",
    "\r\n",
    "#     # standarize and rescale\r\n",
    "#     # standarize\r\n",
    "#     dict_standarized_data = {}\r\n",
    "\r\n",
    "#     for col in selected_cols:\r\n",
    "#       # select data\r\n",
    "#       data = dict_data[col].copy()\r\n",
    "\r\n",
    "#       # scale data\r\n",
    "#       max_val = np.max(data.values)\r\n",
    "#       min_val = np.min(data.values)\r\n",
    "#       data = (data - min_val) / (max_val - min_val)\r\n",
    "\r\n",
    "#       # standarize each var\r\n",
    "#       for var in dict_data[col]:\r\n",
    "#         data[var] = (data[var] - data[var].mean())\r\n",
    "\r\n",
    "#       dict_standarized_data[col] = data\r\n",
    "    \r\n",
    "#     # from PCA time dimension univariate\r\n",
    "#     # cumulative GDD: n_components = 6\r\n",
    "#     # cumulative PREC: n_components = 7\r\n",
    "\r\n",
    "#     from sklearn.decomposition import PCA\r\n",
    "\r\n",
    "#     arr_nb_PC = [6, 7, 8, 8]\r\n",
    "#     # where to store pca\r\n",
    "#     dict_pca = {}\r\n",
    "#     # where to store transformed data\r\n",
    "#     dict_transformed_data = {}\r\n",
    "\r\n",
    "#     # loop over weather vars\r\n",
    "#     for col, n in zip(selected_cols, arr_nb_PC):\r\n",
    "#       # build pca\r\n",
    "#       pca = PCA(n_components=n)\r\n",
    "#       pca.fit(dict_standarized_data[col])\r\n",
    "\r\n",
    "#       dict_pca[col] = pca\r\n",
    "    \r\n",
    "#       # transform data\r\n",
    "#       dict_transformed_data[col] = pd.DataFrame(pca.transform(dict_standarized_data[col]), index=dict_standarized_data[col].index)\r\n",
    "\r\n",
    "#     # join transformed data sets\r\n",
    "#     merged_data = dict_transformed_data[\"cumulative_GDD\"].join(dict_transformed_data[\"cumulative_PRECTOT\"], rsuffix='_other')\r\n",
    "\r\n",
    "#     for col in selected_cols[2:]:\r\n",
    "#         merged_data = merged_data.join(dict_transformed_data[col], rsuffix='_other_')\r\n",
    "    \r\n",
    "#     # perform the clustering\r\n",
    "#     from sklearn.cluster import AgglomerativeClustering\r\n",
    "\r\n",
    "#     X = merged_data\r\n",
    "#     model = AgglomerativeClustering(n_clusters=nb_clusters, linkage=linkage_method, affinity=metric)\r\n",
    "#     model = model.fit(X)\r\n",
    "\r\n",
    "#     labels = model.fit_predict(X)\r\n",
    "\r\n",
    "#     # isolate clusters (according to precipitation)\r\n",
    "#     # to re name cluster { High, Medium, Low }\r\n",
    "#     col_name = \"cumulative_PRECTOT\"\r\n",
    "#     data_cluster = dict_data[col_name]\r\n",
    "\r\n",
    "#     data_cluster[f\"cluster_{col_name}\"] = labels\r\n",
    "\r\n",
    "#     # isolate each cluster\r\n",
    "#     dict_cluster = {}\r\n",
    "#     # to store max val of each cluster\r\n",
    "#     arr_max_vals = []\r\n",
    "\r\n",
    "#     for cluster_index in range(nb_clusters):\r\n",
    "#         query = data_cluster[f\"cluster_{col_name}\"] == cluster_index\r\n",
    "\r\n",
    "#         # drop col of cluster\r\n",
    "#         copy_data = data_cluster[query].copy()\r\n",
    "#         copy_data = copy_data.drop(labels=[f\"cluster_{col_name}\"], axis=1)\r\n",
    "\r\n",
    "#         dict_cluster[cluster_index] = copy_data\r\n",
    "#         arr_max_vals.append(\r\n",
    "#             (cluster_index, np.max(copy_data.values))\r\n",
    "#             )\r\n",
    "\r\n",
    "#     # give name to clusters { High, medium, low }\r\n",
    "#     arr_max_vals.sort(key=lambda item: item[1], reverse=True)\r\n",
    "\r\n",
    "#     for key, new_cluster_name in zip([item[0] for item in arr_max_vals], [\"High\", \"Medium\", \"Low\"]):\r\n",
    "#       dict_cluster[new_cluster_name] = dict_cluster.pop(key)\r\n",
    "\r\n",
    "#     # return dict { cluster_name: arr_years }\r\n",
    "#     return { key: val.index for key, val in dict_cluster.items() }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from py_scripts.clustering import multivariate_clustering\r\n",
    "\r\n",
    "multivariate_clustering(data_beni_mellal)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'High': Int64Index([1982, 1987, 1991, 1996, 1997, 2009, 2010, 2011, 2013], dtype='int64'),\n",
       " 'Medium': Int64Index([1988, 1989, 1990, 1994, 1998, 2001, 2003, 2004, 2006, 2008, 2012,\n",
       "             2015],\n",
       "            dtype='int64'),\n",
       " 'Low': Int64Index([1983, 1984, 1985, 1986, 1992, 1993, 1995, 1999, 2000, 2002, 2005,\n",
       "             2007, 2014, 2016, 2017, 2018, 2019, 2020],\n",
       "            dtype='int64')}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# {'High': Int64Index([1982, 1987, 1991, 1996, 1997, 2009, 2010, 2011, 2013], dtype='int64'),\r\n",
    "#  'Medium': Int64Index([1988, 1989, 1990, 1994, 1998, 2001, 2003, 2004, 2006, 2008, 2012,\r\n",
    "#              2015],\r\n",
    "#             dtype='int64'),\r\n",
    "#  'Low': Int64Index([1983, 1984, 1985, 1986, 1992, 1993, 1995, 1999, 2000, 2002, 2005,\r\n",
    "#              2007, 2014, 2016, 2017, 2018, 2019, 2020],\r\n",
    "#             dtype='int64')}"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "1e915f0a29dc84041eaeb02b7b1a21c440e37a87b61d44d5e84a515737dc82bc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}